{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 금융을 위한 대체 데이터: 범주와 사용 사례\n",
    "- 재량적 및 알고리즘 전략을 위한 연료로서 훨씬 다양한 데이터 소스의 최근 출현에 대해 진행\n",
    "- 다양한 디지털 데이터의 가용성과 관리 능력은 투자 산업을 퐇마해 모든 산업에 걸쳐 혁신을 주도하고 있는 머신러닝의 극적인 성능 향상이 중요한 원동력으로 작용하고 있음\n",
    " \n",
    "- 3장에서 다루는 내용\n",
    "    - 대체 데이터 혁명으로 인해 어떤 새로운 정보 소스가 공개됐는가?\n",
    "    - 개인, 비즈니스 프로세스, 센서가 어떻게 대체 데이터를 생성하는가?\n",
    "    - 알고리즘 트레이딩용으로 사용되는 대체 데이터의 공급 증가를 어떻게 평가하는가?\n",
    "    - 웹 스크래핑과 같이 파이썬으로 대체 데이터를 어떻게 작업하는가?\n",
    "    - 대체 데이터의 중요 범주는 무엇이고 대체 데이터 제공업체는 어디인가?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대체 데이터 혁명\n",
    "- 디지털화, 네트워킁, 저장 비용의 급락으로 인한 데이터 홍수는 예측 분석을 가능하게 만드는 정보의 본질에 엄청난 질적 변화를 가져다줬다. (5V)\n",
    "    - 크기(Volume)\n",
    "        - 생성, 수집 저장되는 데이터의 양은 온라인 및 오프라인 활동, 거래, 기록, 다른 원천의 부산물보다 훨씬 큼\n",
    "        - 분석과 저장을 위한 용량 증가와 함께 계속 증가함\n",
    "\n",
    "    - 속도(Velocity)\n",
    "        - 데이터가 생성, 전송, 처리돼 근처에서 혹은 바로, 실시간 속도로 사용할 수 있게 됐다.\n",
    "\n",
    "    - 다양성(Variety)\n",
    "        - 정형화된 CSV 파일이나 관계형 데이터베이스 테이블과 같은 표 형태로만 구성되지 않음\n",
    "        - json이나 HTML과 같은 반정형 데이터가 생성되고, 텍스트 이미지, 오디오, 비디오 데이터 와 같은 비정형 데이터를 생성되 사용할 수 있게 됐다.\n",
    "    \n",
    "    - 정확성(Veracity)\n",
    "        - 소스와 포맷의 다양성으로 인해 데이터 정보 내용의 신뢰성을 검증하는 것이 더 어렵게 됐다.\n",
    "\n",
    "    - 가치(Value)\n",
    "        - 새로운 데이터 세트의 가치를 결정하는 것은 이전보다 더 불확실성을 더해 훨씬 더 시간과 자원이 소요될 수 있다.\n",
    "\n",
    "- 새로운 데이터 소스를 통해 사용한 사례 (예시)\n",
    "    - 대표적인 재화와 서비스 세트에 대한 온라인 가격 데이터로 인플레이션을 측정\n",
    "    - 매장 방문 횟수나 구매 횟수는 회사나 산업 고유의 판매나 경제 활동의 실시간 추정을 가능하게 해줌\n",
    "    - 인공위성 이미지는 이 정보가 다른 곳에서 이용되기 전에 수확량이나 광산, 석유 굴축장에서 활동을 포착할 수 있었음\n",
    "\n",
    "- 다양한 데이터 세트를 처리하고 통합해 머신러닝에 적용할 수 있으면 새로운 인사이트를 도출할 수 있게 된다.\n",
    "    - 과거 : 계량적인 접근 방법 단순 휴리스틱에 의존해 주가 순자산 비율(PBR)과 같은 과거 데이터를 통해 회사를 파악\n",
    "    - 머신러닝 : 새로운 데이터를 통해 얻는 지표를 합성해 진화하는 시장 데이터를 고려한 규칙을 배우고 채택함\n",
    "\n",
    "    - 이러한 인사이트는 가치, 모멘텀, 퀄리티, 감성과 같은 전통적 투자를 포착하고자 새로운 기회를 창출해주고 있음\n",
    "        1. 모멘텀 (Momentum)\n",
    "            - 머신러닝은 시장 가격 변동, 산업 심리, 경제 팩터에 대한 자산 노출을 식별할 수 있다.\n",
    "        2. 가치 (Value)\n",
    "            - 알고리즘은 회사의 본질 가치를 예측하기 위한 재무제표 이외에 많은 양의 경제 및 산업별 정형, 비정형 데이터를 분석할 수 있다.\n",
    "        3. 퀄리티 (Quality)\n",
    "            - 통합 데이터의 정교한 분석을 통해 고객 평가 혹은 직원 리뷰, 전자상거래, 앱 트래픽이 시장 점유율 또는 기타 기초 수익 질적 팩터로 이익을 식별할 수 있다.\n",
    "        4. 감성 (Sentiment)\n",
    "            - 뉴스 및 소셜 미디어 콘텐츠의 실시간 처리 및 해석을 통해 머신러닝 알고리즘은 새로운 감성을 빠르게 감지하고 다양한 소스의 정보를 좀 더 일관된 큰 그림으로 합성할 수 있다.\n",
    "\n",
    "> - 실제 가치 있는 신호가 포함된 데이터는 자유롭게 사용할 수 없으며, 주식 거래 이외의 목적으로 생성됨  \n",
    "> - 대체 데이터 세트는 거래 가능한 신호를 잡아내고 철저한 평가, 비싼 자료 입수, 신중한 관리 복잡한 분석이 요구된다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대체 데이터의 원천\n",
    "- 대체 데이터 세트는 많은 원천에서 생성될 수 있으나 다음과 같이 주로 많이 생성되는 높은 수준에서 분류될 수 있다.  \n",
    "\n",
    "    - 개인  \n",
    "    : 소셜 미디어 게시, 상품 리뷰를 올리거나 검색 엔진을 사용하는 개인 데이터\n",
    "        - 트위터, 페이스북, 링크드인과 같은 범용 사이트에 올려진 게시물과 댓글 같은 소셜미디어 게시물\n",
    "        - 아마존이나 웨이페어와 같은 사이트에서 제품의 관심이나 인식을 반영하는 전자상거래 활동\n",
    "        - 구글이나 빙과 같은 플랫폼을 사용하는 검색 엔진 활동\n",
    "        - 스마트폰 앱 사용, 다운로드, 리뷰\n",
    "        - 메시지 전송 내역(트래픽)과 같은 개인 데이터  \n",
    "<br>\n",
    "    - 비즈니스  \n",
    "    : 상업 거래를 기록(신용카드 데이터)하거나 중개 회사로 공급체인 활동을 포착  \n",
    "    : 훨씬 낮은 빈도로 이용할 수 있는 활동에 대한 선행지표로 매우 효과적을로 활용할 수 있음\n",
    "        - 정보 처리나 금융기관에 의해 구매에 이용할 수 있는 결제 카드(신용, 직불) 거래 데이터\n",
    "        - 은행 기록, 은행원 스캐너 데이터, 공급 체인 주문서와 같은 일반적인 디지털 활동 또는 기록 보관에 의해 생성되는 기업 생성 데이터\n",
    "        - 거래 흐름 및 시장 미시 구조 데이터\n",
    "        - 유동성과 신용도를 평가하는 신용평가 회사나 금융기관에 의해 모니터링 되는 회사 지급금\n",
    "<br>\n",
    "    - 센서  \n",
    "    : 다른 많은 것 중에서 위성 사진, 보안 카메라로부터 이미지나 이동전화 기지국처럼 사람들의 움직임 패턴을 통해 경제 활동을 포착하는 센서 데이터  \n",
    "    : 다양한 장치에 내장된 네트워크 센서는 스마트폰의 확산과 인공위성 기술 비용의 감소로 인해 가장 빠르게 성장하는 데이터 소스  \n",
    "    : 일반적으로 정형화되어 있지 않으며 데이터의 크기가 크지만, 데이터 처리에 높은 문제가 있음  \n",
    "        - 건설, 운송, 상품 공급과 같은 경제 행위를 모니터링하기 위한 위성 이미지\n",
    "        - 자원자의 스마트폰 데이터를 이용해 소매점 트래픽을 추적하거나 선박 혹은 트럭과 같은 운송 노선에서 생성된 지리 위치 데이터\n",
    "        - 관심 가는 장소에 배치한 카메라\n",
    "        - 날씨나 오염 감지 센서\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대체 데이터 평가를 위한 기준\n",
    "- 대체 데이터의 궁극적인 목표는 알파, 즉 양(+)이며 상관관계가 없는 투자 수익을 창출하는 트레이딩 신호에 대한 경쟁력 있는 검색에서 정보 이점을 제공하는 것\n",
    "- 대체 데이터 세트에서 추출한 신호는 독립적으로 또는 퀀트 전략의 일부분으로 다른 신호와 결합해 사용할 수 있음\n",
    "- 단일 데이터 세트에 기반을 둔 전략으로 생성된 샤프 비율(Sharp ratio)이 충분히 높다면 독립적인 사용이 가능하지만 실제로는 드문 경우\n",
    "- 퀀트사는 알파 팩터 라이브러리를 구축해 개별적으로 약한 신호를 내는 팩터들을 결합해서 매력적인 수익율을 만들고자 함\n",
    "\n",
    "- 신호 내용의 질\n",
    "    - 신호 내용은 대상 자산군, 투자 스타일, 전통적인 리스크 프리미엄과의 관계, 가장 중요한 알파 내용과 관려해 평가할 수 있음\n",
    "        - 자산군  \n",
    "            : 대부분의 대체 데이터 세트는 주식과 원자재와 밀접하게 관련된 정보가 포함돼 있음\n",
    "        - 투자 스타일  \n",
    "            : 대체 데이터 세트는 특정 섹터와 주식에 초점을 맞추고 있기 때문에 롱/숏 전략을 실행하는 주식 투자자에게 자연적으로 더 어필하고 있음\n",
    "        - 리스크 프리미엄  \n",
    "            : 대체 데이터로 도출한 신호를 전통적인 리스크 팩터에 기반을 둔 알고리즘 트레이닝과 결합하면 더 분산화된 리스크 프리미엄 포트폴리오를 구축하는 중요한 구성요소로 활용  \n",
    "        - 알파의 내용과 질  \n",
    "            : 대체 데이터 세트는 독자적으로 전략을 도출하기 위한 알파 신호를 충분히 포함할 수 있지만 다양한 대체 및 데이터를 결합하여 사용하는 것이 일반적임\n",
    "\n",
    "- 데이터의 질\n",
    "    - 데이터 세트의 질은 데이터를 분석하고 수익을 창출하는 데 요구되는 노력에 영향을 미치고, 데이터에 포함된 예측 신호의 신뢰성에 영향을 미치기 때문에 또 다른 중요한 기준임\n",
    "    - 질적 측면에서 데이터 빈도와 이용 가능 깃간의 길이, 데이터에 포함된 정보의 신뢰성과 정확성, 현재 또는 향후 미래 규정을 준수하는 범위, 독립적인 사용 방법이 포함됨\n",
    "\n",
    "        - 법적/평판 리스크  \n",
    "            - 법률 및 규정 준수 요구 사항을 철저히 검토해야 함\n",
    "            : 중요 비공개 정보(MNPI) : 내구 거래의 규제 침해  \n",
    "            : 개인 식별 정보(PII) : 주로 유렵언협이 일반 데이터 보호 규정을 제정\n",
    "        - 독점력\n",
    "            - 대체 데이터 세트의 좋은 예측력 있고, 높은 샤프 비율은 대체 데이터의 가용성과 처리 용이성에 반비례함\n",
    "            - 독점력이 있고, 데이터 처리가 어려울수록 알파 창출 능력은 가진 데이터 세트는 빠른 신호 붕괴를 겪지 않고 전략을 추진할 가능성이 높음\n",
    "        - 투자 기간\n",
    "            - 다양한 시나리오에서 데이터 세트의 예측력을 검증하는 데는 광범위한 시간이 매우 바람직함\n",
    "        - 데이터 빈도\n",
    "            - 데이터 빈도는 새로운 정보를 얼마나 자주 이용할 수 있는지, 주어진 기간에 예측 신호가 어떻게 차별화되는지를 결정\n",
    "        - 데이터 신뢰성\n",
    "            - 데이터가 측정하고자 하는 바를 정확히 반영하고 있는지, 얼마나 잘 검증할 수 있는지는 중요한 관심사며 철저한 감사를 통해 검증해야 함\n",
    "            - 분석될 대상에 대한 정보 추출이나 사용된 방법론이 원시 및 가공 데이터에 둘 다 모두 적용함\n",
    "\n",
    "\n",
    "- 기술적 측면\n",
    "    - 레이턴시(latency : 지연시간) 또는 보고의 지연 데이터의 이용 가능한 형식이 중요\n",
    "        - 레이턴시\n",
    "            : 데이터 수집 방법, 후속 처리와 전송, 규제 또는 법적 제약으로 인해 레이턴시가 발생할 수 있음\n",
    "        - 데이터 형식\n",
    "            : 데이터는 다양한 형식으로 제공되며, 처리된 데이터는 사용하기 편한 형식이며 견고한 API를 통해 기존 시스템이나 쿼리에 쉽게 통합됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대체 데이터 시장\n",
    "- 데이터 제공업체와 사용 사례\n",
    "    다양한 데이터 제공업체들이 있으며, 소셜 감정 분석은 가장 큰 범주이며, 최근 몇 년 동안 인공위성과 지리적 위치 데이터가 빠르게 성장하고 있음  \n",
    "    <img src='./img/2-01.png' width=\"340px\" height =\"500px\" alt=\"데이터 제공업체\"></img>\n",
    "\n",
    "- 소셜 감성 데이터\n",
    "    - 주로 트위터 데이터와 밀접하게 관련있음\n",
    "        - 데이터 마이너 : 트위터와 독점적인 계약을 맺어 소셜 감정 데이터와 뉴스 분석을 제공\n",
    "        - 스톡트윗 : 수십만 명의 투자 전문가가 스톡트윗 형태로 정보 및 거래 아이디어를 공유하는 소셜 네트워크 및 마이크로 블로깅 플랫폼\n",
    "        - 레이븐팩 : 투자자와 관련 있은 정보를 전달하기 위한 목적으로 감성 점수를 포함한 정형화된 지표를 제공하고자 다양한 비정형 텍스트 기반 데이터를 분석\n",
    "\n",
    "- 위성 데이터\n",
    "    - 2010년에 설립된RS 매트릭스는 부동산, 산업용 애플리케이션뿐만 아니라 금속, 원자재까지 초점을 맞춰 데이터를 수집 제공하고 있음\n",
    "    - 자체 소유한 고해상도 위성을 기반으로 신호, 예측 분석, 경고, 최종 사용자 애플리케이션을 제공함\n",
    "    - (사용 사례)  \n",
    "    특정 일반 금속의 생산 및 저장, 관련 생산 위치에서 고용뿐만 아니라 특정 체인의 소매 트래픽 추정 또는 상업용 부동산\n",
    "\n",
    "- 위치 정보 데이터\n",
    "    - 2015년에 설립된 애드번은 스마트폰 트랙픽 데이터에서 도출된 신호를 헤지 펀드 고객에게 제공하고 있으며, 미국과 EU의 다양한 분야에서 1,600개의 티커를 대상으로함\n",
    "    - 명시적인 사용자 동의하에 스마트폰 위치 정보 코드를 설치한 앱을 사용해 데이터를 수집하고 정확도를 높이고자 여러 챌널(ex. 와이파이, 블루투스, 셀룰러 신호)을 통해 위치를 추적\n",
    "    - (사용 사례)  \n",
    "    실제 매장에서 발생된 고객 트래픽을 추정해 상장된 회사의 매출액을 예측하는 모델에 입력 변수로 활용될 수 있음\n",
    "\n",
    "- 이메일 영수증 데이터\n",
    "    - 이글 알파는 5,000개 이상의 소매업체를 대상으로 하는 이메일 영수증을 사용해 대규모 온라인 거래 데이터 세트를 제공함\n",
    "    - 53개 제품 그룹에서 분류한 SKU 수준의 거래 데이터를 제공\n",
    "    - 데이터 세트에는 전체 합계 지출, 주문수, 기간별 고유 구매자 수 등이 포함됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대체 데이터로 작업\n",
    "\n",
    "- 오픈 테이블 데이터 스크래핑\n",
    "    - 대체 데이터의 일반적인 소스는 직원의 의견이나 고객 리뷰를 사용해 내부자 통찰력을 전달하는 글래스더오와 엘프 같은 비류 웹사이트를 말함\n",
    "    - 사용자가 제공한 콘텐츠는 대표적 관점을 포착하는 것이 아니라 심각한 선택 편향에 영향을 받을 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bs4와 request를 사용한 HTML에서 데이터 추룰\n",
    "    1. Request 라이브러리를 이용해 HTTP(Hypertext transfer Protocol)요청을 만들고 HTML 소스코드 검색\n",
    "    2. Beautiful Soup 라이브러리를 사용해 HTML 마크업 코드를 쉽게 구문 분석하고 관심 있는 텍스트 콘텐츠를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL 설정과 요청(request); 소스코드 추출\n",
    "url = \"https://www.opentable.com/new-york-restaurant-listings\"\n",
    "req = requests.get(url)\n",
    "print(req.text[:100])\n",
    "\n",
    "# soup 객체\n",
    "soup = BeautifulSoup(req.text, 'html-parser')\n",
    "\n",
    "# 태그 파싱\n",
    "for entry in soup.find_all(name='span', atrs=({'class' : 'rest-row-name-text'})):\n",
    "    print(entry.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스크래피(Scrapy)는 링크를 따라가는 봇을 구축하고, 컨텐츠를 검색하고, 구문 분석된 결과를 구조화된 방식으로 저장하는 강력한 라이브러리\n",
    "\n",
    "- 스플래시(splash) 헤드리스 브라우저와 함께 사용하면 자바스크립트를 해석할 수 있으며 Selenium의 효율적인 대안이 됨\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Field, Item\n",
    "from scrapy import Spider\n",
    "from scrapy_splash import SplashRequest\n",
    "\n",
    "\n",
    "class OpentableItem(Item):\n",
    "    name = Field()\n",
    "    price = Field()\n",
    "    bookings = Field()\n",
    "    cuisine = Field()\n",
    "    location = Field()\n",
    "    reviews = Field()\n",
    "    rating = Field()\n",
    "\n",
    "\n",
    "class OpenTableSpider(Spider):\n",
    "    name = 'opentable'\n",
    "    start_urls = ['https://www.opentable.com/new-york-restaurant-listings']\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield SplashRequest(url=url,\n",
    "                                callback=self.parse,\n",
    "                                endpoint='render.html',\n",
    "                                args={'wait': 1},\n",
    "                                )\n",
    "\n",
    "    def parse(self, response):\n",
    "        item = OpentableItem()\n",
    "        for resto in response.css('div.rest-row-info'):\n",
    "            item['name'] = resto.css('span.rest-row-name-text::text').extract()\n",
    "            item['bookings'] = resto.css('div.booking::text').re(r'\\d+')\n",
    "            item['rating'] = resto.css('div.all-stars::attr(style)').re_first('\\d+')\n",
    "            item['reviews'] = resto.css('span.star-rating-text--review-text::text').re_first(r'\\d+')\n",
    "            item['price'] = len(resto.css('div.rest-row-pricing > i::text').re('\\$'))\n",
    "            item['cuisine'] = resto.css('span.rest-row-meta--cuisine::text').extract()\n",
    "            item['location'] = resto.css('span.rest-row-meta--location::text').extract()\n",
    "            yield item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 어닝콜 트랜스크립트 스크래핑과 파싱 (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html):\n",
    "    \"\"\"Main html parser function\"\"\"\n",
    "    date_pattern = re.compile(r'(\\d{2})-(\\d{2})-(\\d{2})')\n",
    "    quarter_pattern = re.compile(r'(\\bQ\\d\\b)')\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    meta, participants, content = {}, [], []\n",
    "    h1 = soup.find('h1', itemprop='headline')\n",
    "    if h1 is None:\n",
    "        return\n",
    "    h1 = h1.text\n",
    "    meta['company'] = h1[:h1.find('(')].strip()\n",
    "    meta['symbol'] = h1[h1.find('(') + 1:h1.find(')')]\n",
    "\n",
    "    title = soup.find('div', class_='title')\n",
    "    if title is None:\n",
    "        return\n",
    "    title = title.text\n",
    "    print(title)\n",
    "    match = date_pattern.search(title)\n",
    "    if match:\n",
    "        m, d, y = match.groups()\n",
    "        meta['month'] = int(m)\n",
    "        meta['day'] = int(d)\n",
    "        meta['year'] = int(y)\n",
    "\n",
    "    match = quarter_pattern.search(title)\n",
    "    if match:\n",
    "        meta['quarter'] = match.group(0)\n",
    "\n",
    "    qa = 0\n",
    "    speaker_types = ['Executives', 'Analysts']\n",
    "    for header in [p.parent for p in soup.find_all('strong')]:\n",
    "        text = header.text.strip()\n",
    "        if text.lower().startswith('copyright'):\n",
    "            continue\n",
    "        elif text.lower().startswith('question-and'):\n",
    "            qa = 1\n",
    "            continue\n",
    "        elif any([type in text for type in speaker_types]):\n",
    "            for participant in header.find_next_siblings('p'):\n",
    "                if participant.find('strong'):\n",
    "                    break\n",
    "                else:\n",
    "                    participants.append([text, participant.text])\n",
    "        else:\n",
    "            p = []\n",
    "            for participant in header.find_next_siblings('p'):\n",
    "                if participant.find('strong'):\n",
    "                    break\n",
    "                else:\n",
    "                    p.append(participant.text)\n",
    "            content.append([header.text, qa, '\\n'.join(p)])\n",
    "    return meta, participants, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_result(meta, participants, content):\n",
    "    \"\"\"Save parse content to csv\"\"\"\n",
    "    path = transcript_path / 'parsed' / meta['symbol']\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(content, columns=['speaker', 'q&a', 'content']).to_csv(path / 'content.csv', index=False)\n",
    "    pd.DataFrame(participants, columns=['type', 'name']).to_csv(path / 'participants.csv', index=False)\n",
    "    pd.Series(meta).to_csv(path / 'earnings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from time import sleep\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from furl import furl\n",
    "from selenium import webdriver\n",
    "from pathlib import Path\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "transcript_path = Path('transcripts')\n",
    "SA_URL = 'https://seekingalpha.com/'\n",
    "TRANSCRIPT = re.compile('Earnings Call Transcript')\n",
    "next_page = True\n",
    "page = 1\n",
    "\n",
    "## Chrome Driver로 변경 (webdriver 설치 하지 않고 Driver 사용할 수 있음)\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "while next_page:\n",
    "    print(f'Page: {page}')\n",
    "    url = f'{SA_URL}/earnings/earnings-call-transcripts/{page}'\n",
    "    driver.get(urljoin(SA_URL, url))\n",
    "    sleep(8 + (random() - .5) * 2)\n",
    "    response = driver.page_source\n",
    "    page += 1\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "    links = soup.find_all(name='a', string=TRANSCRIPT)\n",
    "    if len(links) == 0:\n",
    "        next_page = False\n",
    "    else:\n",
    "        for link in links:\n",
    "            transcript_url = link.attrs.get('href')\n",
    "            article_url = furl(urljoin(SA_URL, transcript_url)).add({'part': 'single'})\n",
    "            driver.get(article_url.url)\n",
    "            html = driver.page_source\n",
    "            result = parse_html(html)\n",
    "            if result is not None:\n",
    "                meta, participants, content = result\n",
    "                meta['link'] = link\n",
    "                store_result(meta, participants, content)\n",
    "                sleep(8 + (random()-.5), 2)\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
