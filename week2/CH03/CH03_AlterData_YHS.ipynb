{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 03. 금융을 위한 대체 데이터: 범주와 사용 사례"
      ],
      "metadata": {
        "id": "tqphVzX_WyRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대체 데이터 혁명\n",
        "- 빅데이터의 5V: 크기, 속도, 다양성, 정확성, 가치\n",
        "\n",
        "- 새로운 데이터 소스의 사용 사례\n",
        "    - 대표적인 재화와 서비스 세트에 대한 온라인 가격 데이터로 인플레이션을 측정\n",
        "    - 매장 방문 횟수나 구매 횟수는 회사나 산업 고유의 판매나 경제 활동의 실시간 추정을 가능하게 함\n",
        "    - 인공위성 이미지는 이 정보가 다른 곳에서 이용되기 전에 수확량이나 광산, 석유 굴착장에서의 활동을 포착\n",
        "\n",
        "- 전통적 투자를 포착하고자 새로운 기회를 창출\n",
        "    - 모멘텀: ML은 시장 가격 변동, 산업 심리, 경제 팩터에 대한 자산 노출을 식별\n",
        "    - 가치: 알고리듬은 회사의 본질 가치를 예측하기 위한 재무제표 이외에 많은 자산 노출을 식별\n",
        "    - 퀄리티: 통합 데이터의 정교한 분석을 통해 고객 평가 혹은 직원 리뷰, 전자 상거래, 앱 트래픽이 시장 점유율 또는 기타 기초 수익 질적 팩터로 이익을 식별\n",
        "    - 감성: 뉴스 및 소셜 미디어 콘텐츠의 실시간 처리 및 해석을 통해 ML 알고리듬은 새로운 감성을 빠르게 감지하고 다양한 소스의 정보를 좀 더 일관된 큰 그림으로 합성"
      ],
      "metadata": {
        "id": "o9SgMt0aW_Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대체 데이터의 원천\n",
        "- 개인: 소셜 미디어 게시, 상품 리뷰를 올리거나 검색 엔진을 사용하는 개인 데이터\n",
        "- 비즈니스: 상업 거래를 기록(특히 신용카드 결제)하거나 중개 회사로 공급체인 활동을 포착\n",
        "- 센서: 다른 많은 것 중에서 위성 사진, 보안 카메라로부터 이미지나 이동전화 기지국처럼 사람들의 움직임 패턴을 통해 경제 활동을 포착하는 센서 데이터\n",
        "---\n",
        "- 개인\n",
        "    - 개인은 온라인 활동을 통해 전자 데이터를 생성\n",
        "- 비즈니스 프로세스\n",
        "    - 비즈니스 프로세스로부터 생성되는 데이터는 개인이 생성한 것보다 많은 구조를 가짐\n",
        "> 판매 시점 데이터(POS 데이터)와 같은 신용카드 거래와 기업 생성 데이터는 가장 신뢰할 만하며 예측력이 있는 데이터 세트\n",
        "- 센서\n",
        "    - 다양한 장치에 내장된 네트워크 센서\n",
        "- 인공위성\n",
        "    - 농업, 광물 생산, 선적, 상업용 또는 주거용 건물, 선박 건조 같은 항공 범위를 사용해 포착할 수 있는 경제 활동 모니터링\n",
        "- 위치 정보 데이터\n",
        "    - 마케팅 활동의 영향을 측정하며, 왕래 또는 판매를 추정하는데 사용될 수 있다."
      ],
      "metadata": {
        "id": "-xXdtIjnX5rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대체 데이터 평가를 위한 기준\n",
        "- 대체 데이터의 궁극적인 목표\n",
        "    - 알파(양+이며 상관관계가 없는 투자 수익)을 창출하는 트레이딩 신호에 대한 경쟁력 있는 검색에서 정보 이점을 제공하는 것\n",
        "\n",
        "대체 데이터 세트는 신호 내용의 품질, 데이터의 질적 측면, 다양한 기술적 측면에서 평가될 수 있다.\n",
        "\n",
        "- 신호 내용의 질\n",
        "    - 자산군\n",
        "    - 투자 스타일\n",
        "    - 리스크 프리미엄\n",
        "    - 앞아의 내용과 질\n",
        "- 데이터의 질\n",
        "    - 법적/평판 리스크\n",
        "    - 독점력\n",
        "    - 투자 기간\n",
        "    - 데이터 빈도\n",
        "    - 데이터 신뢰성\n",
        "- 기술적 측면\n",
        "    - 레이턴시\n",
        "    - 데이터 형식"
      ],
      "metadata": {
        "id": "DtATlTUaZQv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대체 데이터 시장\n",
        "### 데이터 제공업체와 사용 사례\n",
        "- 소셜 감성 데이터\n",
        "    - 지닙\n",
        "    - 데이터마이너\n",
        "    - 스톡트윗\n",
        "    - 레이븐 팩\n",
        "- 위성 데이터\n",
        "    - RS 매트릭스\n",
        "- 위치 정보 데이터\n",
        "    - 애드번\n",
        "- 이메일 영수증 데이터\n",
        "    - 이글 알파\n",
        "    "
      ],
      "metadata": {
        "id": "ZRjcE65GadlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대체 데이터로 작업\n",
        "- 웹 스크리핑을 사용해 대체 데이터를 수집"
      ],
      "metadata": {
        "id": "cAUB5QKjbRx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 오픈테이블 데이터 스크래핑"
      ],
      "metadata": {
        "id": "SZ6BWqQMcJhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r81ruZxigCNh",
        "outputId": "623c52b9-eac7-40f0-e6e5-52f7a9a6e887"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPghIRi6WinO",
        "outputId": "0dce147a-c456-4571-d853-9b2e9b4e1ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-11 12:49:02--  https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/31e07152-f930-40e0-8011-5495dd63fee9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230611%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230611T124902Z&X-Amz-Expires=300&X-Amz-Signature=104f8f5d3fabdac7a21371cf66f917583762ccb62597d9b7356dd0f991bd31a6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.33.0-linux64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-06-11 12:49:02--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/31e07152-f930-40e0-8011-5495dd63fee9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230611%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230611T124902Z&X-Amz-Expires=300&X-Amz-Signature=104f8f5d3fabdac7a21371cf66f917583762ccb62597d9b7356dd0f991bd31a6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.33.0-linux64.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3074828 (2.9M) [application/octet-stream]\n",
            "Saving to: ‘geckodriver-v0.33.0-linux64.tar.gz’\n",
            "\n",
            "\r          geckodriv   0%[                    ]       0  --.-KB/s               \rgeckodriver-v0.33.0 100%[===================>]   2.93M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-06-11 12:49:02 (228 MB/s) - ‘geckodriver-v0.33.0-linux64.tar.gz’ saved [3074828/3074828]\n",
            "\n",
            "geckodriver\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf /content/geckodriver-*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnD11CVbgPOY",
        "outputId": "34d050fb-7179-4680-a884-7e7a2f443469"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "geckodriver\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x geckodriver\n",
        "!sudo mv geckodriver /usr/local/bin/"
      ],
      "metadata": {
        "id": "rqjYagaAgWOZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install firefox\n",
        "!apt install -y firefox-geckodriver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcXak9C7d5bX",
        "outputId": "7c6ec051-0037-47c7-8207-af98fc33058f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.10.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.26.15)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.22.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.10.3)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.10)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,776 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,255 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,354 kB]\n",
            "Fetched 7,725 kB in 2s (4,129 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libdbus-glib-1-2 libdbusmenu-glib4 libdbusmenu-gtk3-4 libxtst6\n",
            "  xul-ext-ubufox\n",
            "Suggested packages:\n",
            "  fonts-lyx\n",
            "The following NEW packages will be installed:\n",
            "  firefox libdbus-glib-1-2 libdbusmenu-glib4 libdbusmenu-gtk3-4 libxtst6\n",
            "  xul-ext-ubufox\n",
            "0 upgraded, 6 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 61.0 MB of archives.\n",
            "After this operation, 245 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libdbus-glib-1-2 amd64 0.110-5fakssync1 [59.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 firefox amd64 114.0+build3-0ubuntu0.20.04.1 [60.8 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libdbusmenu-glib4 amd64 16.04.1+18.10.20180917-0ubuntu6 [41.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libdbusmenu-gtk3-4 amd64 16.04.1+18.10.20180917-0ubuntu6 [27.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 xul-ext-ubufox all 3.4-0ubuntu1.17.10.1 [3,320 B]\n",
            "Fetched 61.0 MB in 2s (25.1 MB/s)\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libdbus-glib-1-2_0.110-5fakssync1_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.110-5fakssync1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package firefox.\n",
            "Preparing to unpack .../2-firefox_114.0+build3-0ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking firefox (114.0+build3-0ubuntu0.20.04.1) ...\n",
            "Selecting previously unselected package libdbusmenu-glib4:amd64.\n",
            "Preparing to unpack .../3-libdbusmenu-glib4_16.04.1+18.10.20180917-0ubuntu6_amd64.deb ...\n",
            "Unpacking libdbusmenu-glib4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Selecting previously unselected package libdbusmenu-gtk3-4:amd64.\n",
            "Preparing to unpack .../4-libdbusmenu-gtk3-4_16.04.1+18.10.20180917-0ubuntu6_amd64.deb ...\n",
            "Unpacking libdbusmenu-gtk3-4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Selecting previously unselected package xul-ext-ubufox.\n",
            "Preparing to unpack .../5-xul-ext-ubufox_3.4-0ubuntu1.17.10.1_all.deb ...\n",
            "Unpacking xul-ext-ubufox (3.4-0ubuntu1.17.10.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up libdbusmenu-glib4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.110-5fakssync1) ...\n",
            "Setting up xul-ext-ubufox (3.4-0ubuntu1.17.10.1) ...\n",
            "Setting up libdbusmenu-gtk3-4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Setting up firefox (114.0+build3-0ubuntu0.20.04.1) ...\n",
            "update-alternatives: using /usr/bin/firefox to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/firefox to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "Please restart all running instances of firefox, or you will experience problems.\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  firefox-geckodriver\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 1,278 kB of archives.\n",
            "After this operation, 4,431 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 firefox-geckodriver amd64 114.0+build3-0ubuntu0.20.04.1 [1,278 kB]\n",
            "Fetched 1,278 kB in 1s (1,873 kB/s)\n",
            "Selecting previously unselected package firefox-geckodriver.\n",
            "(Reading database ... 122670 files and directories currently installed.)\n",
            "Preparing to unpack .../firefox-geckodriver_114.0+build3-0ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking firefox-geckodriver (114.0+build3-0ubuntu0.20.04.1) ...\n",
            "Setting up firefox-geckodriver (114.0+build3-0ubuntu0.20.04.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIsX42bzeKeO",
        "outputId": "c089cfcc-cf5d-4487-cdcb-06ade1890681"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-3.8.6-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.27.1)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.4)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.0 webdriver_manager-3.8.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "\n",
        "import re\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "\n",
        "\n",
        "# 클래스가 암호화되서 직접 찾아가며 크롤링 해야함\n",
        "def parse_html(page_source):\n",
        "    \"\"\"Parse content from various tags from OpenTable restaurants listing\"\"\"\n",
        "    data = []\n",
        "    soup = BeautifulSoup(page_source, 'html.parser')\n",
        "    for resto in soup.find_all('div', class_='rest-row-info'):\n",
        "        item = {}\n",
        "        item['name'] = resto.find('span', class_='rest-row-name-text').text\n",
        "\n",
        "        booking = resto.find('div', class_='booking')\n",
        "        item['bookings'] = re.search(r'\\d+', booking.text).group() if booking else 'NA'\n",
        "\n",
        "        rating = resto.find('div', class_='star-rating-score')\n",
        "        item['rating'] = float(rating['aria-label'].split()[0]) if rating else 'NA'\n",
        "\n",
        "        reviews = resto.find('span', class_='underline-hover')\n",
        "        item['reviews'] = int(re.search(r'\\d+', reviews.text).group()) if reviews else 'NA'\n",
        "\n",
        "        pricing = resto.find('div', class_='rest-row-pricing')\n",
        "        item['price'] = pricing.find_all('i').count('$') if pricing else 0\n",
        "\n",
        "        cuisine = resto.find('span', class_='rest-row-meta--cuisine rest-row-meta-text sfx1388addContent')\n",
        "        item['cuisine'] = cuisine.text.strip() if cuisine else ''\n",
        "\n",
        "        location = resto.find('span', class_='rest-row-meta--location rest-row-meta-text sfx1388addContent')\n",
        "        item['location'] = location.text.strip() if location else ''\n",
        "\n",
        "        data.append(item)\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "options = Options()\n",
        "options.add_argument('--headless')  # 브라우저 창을 표시하지 않습니다.\n",
        "\n",
        "# Start selenium and click through pages until reach end\n",
        "# store results by iteratively appending to csv file\n",
        "driver = webdriver.Firefox(options=options)\n",
        "url = \"https://www.opentable.com/new-york-restaurant-listings\"\n",
        "driver.get(url)\n",
        "page = collected = 0\n",
        "while True:\n",
        "    sleep(1)\n",
        "    new_data = parse_html(driver.page_source)\n",
        "    print(new_data)\n",
        "    if new_data.empty:\n",
        "        break\n",
        "    if page == 0:\n",
        "        new_data.to_csv('results.csv', index=False)\n",
        "    elif page > 0:\n",
        "        new_data.to_csv('results.csv', index=False, header=None, mode='a')\n",
        "    page += 1\n",
        "    collected += len(new_data)\n",
        "    print(f'Page: {page} | Downloaded: {collected}')\n",
        "    driver.find_element_by_link_text('Next').click()\n",
        "\n",
        "driver.close()\n",
        "restaurants = pd.read_csv('results.csv')\n",
        "print(restaurants)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V4AkhQhdtO-",
        "outputId": "4d4e4aac-e49c-4528-9348-7a2eb6c5eec0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 어닝콜 트랜스크립트 스크래핑과 파싱"
      ],
      "metadata": {
        "id": "ujJeQbAejt_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install furl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNZ1C111kDAw",
        "outputId": "7fad49e2-fa29-4092-ba11-373a203748f5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting furl\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from furl) (1.16.0)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl)\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: orderedmultidict, furl\n",
            "Successfully installed furl-2.1.3 orderedmultidict-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir parsed"
      ],
      "metadata": {
        "id": "R3VMYw08lAG_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from random import random\n",
        "from time import sleep\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from furl import furl\n",
        "from selenium import webdriver\n",
        "\n",
        "# transcript_path = Path('/content/')\n",
        "\n",
        "def store_result(meta, participants, content):\n",
        "    \"\"\"Save parse content to csv\"\"\"\n",
        "    path = '/content/parsed/'+meta['symbol'] # 경로 수동 지정\n",
        "    if not path.exists():\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "    pd.DataFrame(content, columns=['speaker', 'q&a', 'content']).to_csv(path / 'content.csv', index=False)\n",
        "    pd.DataFrame(participants, columns=['type', 'name']).to_csv(path / 'participants.csv', index=False)\n",
        "    pd.Series(meta).to_csv(path / 'earnings.csv')\n",
        "\n",
        "\n",
        "# 클래스가 암호화되서 직접 찾아가며 크롤링 해야함\n",
        "def parse_html(html):\n",
        "    \"\"\"Main html parser function\"\"\"\n",
        "    date_pattern = re.compile(r'(\\d{2})-(\\d{2})-(\\d{2})')\n",
        "    quarter_pattern = re.compile(r'(\\bQ\\d\\b)')\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "    meta, participants, content = {}, [], []\n",
        "    h1 = soup.find('h1', itemprop='headline')\n",
        "    if h1 is None:\n",
        "        return\n",
        "    h1 = h1.text\n",
        "    meta['company'] = h1[:h1.find('(')].strip()\n",
        "    meta['symbol'] = h1[h1.find('(') + 1:h1.find(')')]\n",
        "\n",
        "    title = soup.find('div', class_='title')\n",
        "    if title is None:\n",
        "        return\n",
        "    title = title.text\n",
        "    print(title)\n",
        "    match = date_pattern.search(title)\n",
        "    if match:\n",
        "        m, d, y = match.groups()\n",
        "        meta['month'] = int(m)\n",
        "        meta['day'] = int(d)\n",
        "        meta['year'] = int(y)\n",
        "\n",
        "    match = quarter_pattern.search(title)\n",
        "    if match:\n",
        "        meta['quarter'] = match.group(0)\n",
        "\n",
        "    qa = 0\n",
        "    speaker_types = ['Executives', 'Analysts']\n",
        "    for header in [p.parent for p in soup.find_all('strong')]:\n",
        "        text = header.text.strip()\n",
        "        if text.lower().startswith('copyright'):\n",
        "            continue\n",
        "        elif text.lower().startswith('question-and'):\n",
        "            qa = 1\n",
        "            continue\n",
        "        elif any([type in text for type in speaker_types]):\n",
        "            for participant in header.find_next_siblings('p'):\n",
        "                if participant.find('strong'):\n",
        "                    break\n",
        "                else:\n",
        "                    participants.append([text, participant.text])\n",
        "        else:\n",
        "            p = []\n",
        "            for participant in header.find_next_siblings('p'):\n",
        "                if participant.find('strong'):\n",
        "                    break\n",
        "                else:\n",
        "                    p.append(participant.text)\n",
        "            content.append([header.text, qa, '\\n'.join(p)])\n",
        "    return meta, participants, content\n",
        "\n",
        "\n",
        "SA_URL = 'https://seekingalpha.com/'\n",
        "TRANSCRIPT = re.compile('Earnings Call Transcript')\n",
        "\n",
        "next_page = True\n",
        "page = 1\n",
        "\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "options = Options()\n",
        "options.add_argument('--headless')  # 브라우저 창을 표시하지 않습니다.\n",
        "\n",
        "# Start selenium and click through pages until reach end\n",
        "# store results by iteratively appending to csv file\n",
        "driver = webdriver.Firefox(options=options)\n",
        "\n",
        "while next_page:\n",
        "    print(f'Page: {page}')\n",
        "    url = f'{SA_URL}/earnings/earnings-call-transcripts/{page}'\n",
        "    driver.get(urljoin(SA_URL, url))\n",
        "    sleep(8 + (random() - .5) * 2)\n",
        "    response = driver.page_source\n",
        "    page += 1\n",
        "    soup = BeautifulSoup(response, 'lxml')\n",
        "    links = soup.find_all(name='a', string=TRANSCRIPT)\n",
        "    if len(links) == 0:\n",
        "        next_page = False\n",
        "    else:\n",
        "        for link in links:\n",
        "            transcript_url = link.attrs.get('href')\n",
        "            article_url = furl(urljoin(SA_URL, transcript_url)).add({'part': 'single'})\n",
        "            driver.get(article_url.url)\n",
        "            html = driver.page_source\n",
        "            result = parse_html(html)\n",
        "            print(result)\n",
        "            if result is not None:\n",
        "                meta, participants, content = result\n",
        "                meta['link'] = link\n",
        "                store_result(meta, participants, content)\n",
        "                sleep(8 + (random() - .5) * 2)\n",
        "\n",
        "driver.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHKRjQ_NhmvS",
        "outputId": "a8e337f3-6250-4072-f434-3e575d10b760"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: 1\n"
          ]
        }
      ]
    }
  ]
}